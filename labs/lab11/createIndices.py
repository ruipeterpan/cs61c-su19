import sys
import re

# https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations

from pyspark import SparkContext,SparkConf

'''
output of flatMap is a list. Each element is a tuple: (word+documentID, index of word in document)
'''
def flatMapFunc(document):
    """
    document[0] is the document ID (distinct for each document)
    document[1] is a string of all text in that document

    You will need to modify this code.
    """
    documentID = document[0]
    words = re.findall(r"\w+", document[1])
    words_list = []
    word_index = 0
    for word in words:
        index_list = []
        index_list.append(word_index)
        words_list.append((word+documentID, index_list))
        word_index += 1
    return words_list

def mapFunc(arg):
    """
    You may need to modify this code.
    """
    return (arg[0], arg[1])

def reduceFunc(arg1, arg2):
    """
    You may need to modify this code.
    """
    return arg1 + arg2

def createIndices(file_name, output="spark-wc-out-createIndices"):
    sc = SparkContext("local[8]", "CreateIndices", conf=SparkConf().set("spark.hadoop.validateOutputSpecs", "false"))
    file = sc.sequenceFile(file_name)

    indices = file.flatMap(flatMapFunc) \
                 .map(mapFunc) \
                 .reduceByKey(reduceFunc) \
                 .sortByKey()

    indices.coalesce(1).saveAsTextFile(output)

""" Do not worry about this """
if __name__ == "__main__":
    argv = sys.argv
    if len(argv) == 2:
        createIndices(argv[1])
    else:
        createIndices(argv[1], argv[2])
